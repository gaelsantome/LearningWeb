<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Historia de la Computación</title>
</head>

<body>
    
    <h1>Historia de la Computación</h1>
    <p>La historia de la computación es un recorrido que abarca siglos de avances tecnológicos, desde los primeros intentos por
    resolver problemas matemáticos hasta la creación de las máquinas modernas que usamos hoy en día. En la antigüedad, las
    herramientas de cálculo como el ábaco, utilizado alrededor del 3000 a.C., fueron algunos de los primeros intentos para
    facilitar los cálculos. Sin embargo, fue en el siglo XIX cuando comenzaron a gestarse los fundamentos de la computación
    moderna. Charles Babbage diseñó la máquina analítica, un dispositivo mecánico programable que, aunque nunca se completó,
    es considerado un precursor clave de las computadoras actuales. Ada Lovelace, en 1843, escribió las primeras
    instrucciones para que la máquina de Babbage realizara cálculos, lo que la convierte en la primera programadora de la
    historia.</p>
    <p>Ya en el siglo XX, durante la Segunda Guerra Mundial, se dieron avances significativos. En 1941, el ingeniero alemán
    Konrad Zuse diseñó la Z3, una de las primeras computadoras electromecánicas que podía realizar cálculos automáticos y
    programables. Simultáneamente, en el Reino Unido, Alan Turing desarrolló el concepto de la máquina de Turing, que
    definió los principios básicos de los algoritmos y la computación. En la misma época, surgieron las primeras
    computadoras electrónicas como Colossus (1943), utilizada para descifrar códigos secretos, y ENIAC (1945), una de las
    primeras computadoras de propósito general. Estos avances fueron fundamentales, aunque las máquinas de esa época eran
    grandes, lentas y costosas.</p>
    <p>A medida que avanzaba el siglo XX, el transistor, inventado en 1947, reemplazó a las costosas válvulas de vacío,
    permitiendo la miniaturización de los circuitos electrónicos y facilitando la creación de computadoras más rápidas y
    pequeñas. En las décadas de 1950 y 1960, las computadoras de gran escala, llamadas mainframes, dominaron las empresas y
    las universidades. En paralelo, surgieron las minicomputadoras, que ofrecían computación más accesible para
    organizaciones de menor tamaño.</p>
    <p>El verdadero cambio hacia las computadoras personales llegó en los años 70 con la invención del microprocesador en 1971
    por Intel, que permitió la creación de computadoras personales a precios más accesibles. El lanzamiento de la IBM PC en
    1981 y el Macintosh de Apple en 1984 marcaron el comienzo de la revolución de las computadoras personales,
    convirtiéndolas en dispositivos comunes en los hogares y oficinas.</p>
        <p>A partir de los años 80 y 90, la Internet comenzó a ganar terreno, conectando computadoras de todo el mundo. El
        desarrollo de la World Wide Web en 1991 por Tim Berners-Lee facilitó la navegación en la red de una manera más accesible
        y visual, lo que transformó la forma en que la humanidad se comunica y comparte información. A partir de ahí, la
        Internet se convirtió en una herramienta esencial en la vida cotidiana, impactando la economía, la cultura y las
        relaciones sociales a nivel global.</p>
        <p>En las últimas décadas, la computación ha experimentado avances aún más vertiginosos con el auge de los smartphones, que
        combinan las capacidades de una computadora con la portabilidad de un teléfono móvil, y la computación en la nube, que
        permite almacenar datos y ejecutar programas de manera remota, eliminando la necesidad de equipos locales potentes.
        Además, tecnologías como el big data y la inteligencia artificial (IA) están transformando industrias enteras, desde la
        medicina hasta la automoción, mejorando la toma de decisiones y la automatización de tareas complejas.</p>
            <p>Mirando al futuro, la computación cuántica se presenta como una de las fronteras más prometedoras. Esta tecnología, que
            utiliza las propiedades de la física cuántica para procesar información, podría revolucionar la velocidad y eficiencia
            de los cálculos. A su vez, la inteligencia artificial continúa evolucionando, con aplicaciones que van desde la mejora
            de la automatización en fábricas hasta los avances en la creación de máquinas capaces de aprender y realizar tareas de
            forma autónoma.</p>
            <p>En resumen, la historia de la computación es una narrativa de innovación continua, que ha llevado de los primeros
            intentos mecánicos de resolver problemas matemáticos a la creación de máquinas poderosas e interconectadas que
            transforman todos los aspectos de la vida moderna.</p>

        <br />
        <button><a href="/exercises/web_html/form.html">Acceder a formulario</a></button>
        <button><a href="/exercises/web_html/table.html">Ver tabla de hitos</a></button>

</body>

</html>